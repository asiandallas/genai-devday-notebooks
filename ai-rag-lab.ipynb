{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Lab Documentation and Solutions](https://img.shields.io/badge/Lab%20Documentation%20and%20Solutions-purple)](https://mongodb-developer.github.io/ai-rag-lab/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup prerequisites\n",
    "\n",
    "Replace `<MONGODB_URI>` with your **MongoDB connection string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ok': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retain the quotes (\"\") when pasting the URI\n",
    "MONGODB_URI = \"mongodb+srv://kyrstnhall:Asian._.dallas02@cluster0.zwlq5.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "# Initialize a MongoDB Python client\n",
    "mongodb_client = MongoClient(MONGODB_URI, appname=\"devrel.workshop.rag\")\n",
    "# Check the connection to the server\n",
    "mongodb_client.admin.command(\"ping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVERLESS_URL = os.environ.get(\"SERVERLESS_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# You may see a warning upon running this cell. You can ignore it.\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the `mongodb-docs` dataset from Hugging Face\n",
    "data = load_dataset(\"mongodb/mongodb-docs\", split=\"train\")\n",
    "# Convert the dataset into a dataframe first, then into a list of Python objects/dictionaries\n",
    "docs = pd.DataFrame(data).to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the number of documents in the dataset\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updated': '2024-05-20T17:30:49.148Z',\n",
       " 'metadata': {'contentType': None,\n",
       "  'productName': 'MongoDB Atlas',\n",
       "  'tags': ['atlas', 'docs'],\n",
       "  'version': None},\n",
       " 'action': 'created',\n",
       " 'sourceName': 'snooty-cloud-docs',\n",
       " 'body': '# View Database Access History\\n\\n- This feature is not available for `M0` free clusters, `M2`, and `M5` clusters. To learn more, see Atlas M0 (Free Cluster), M2, and M5 Limits.\\n\\n- This feature is not supported on Serverless instances at this time. To learn more, see Serverless Instance Limitations.\\n\\n## Overview\\n\\nAtlas parses the MongoDB database logs to collect a list of authentication requests made against your clusters through the following methods:\\n\\n- `mongosh`\\n\\n- Compass\\n\\n- Drivers\\n\\nAuthentication requests made with API Keys through the Atlas Administration API are not logged.\\n\\nAtlas logs the following information for each authentication request within the last 7 days:\\n\\n<table>\\n<tr>\\n<th id=\"Field\">\\nField\\n\\n</th>\\n<th id=\"Description\">\\nDescription\\n\\n</th>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nTimestamp\\n\\n</td>\\n<td headers=\"Description\">\\nThe date and time of the authentication request.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nUsername\\n\\n</td>\\n<td headers=\"Description\">\\nThe username associated with the database user who made the authentication request.\\n\\nFor LDAP usernames, the UI displays the resolved LDAP name. Hover over the name to see the full LDAP username.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nIP Address\\n\\n</td>\\n<td headers=\"Description\">\\nThe IP address of the machine that sent the authentication request.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nHost\\n\\n</td>\\n<td headers=\"Description\">\\nThe target server that processed the authentication request.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nAuthentication Source\\n\\n</td>\\n<td headers=\"Description\">\\nThe database that the authentication request was made against. `admin` is the authentication source for SCRAM-SHA users and `$external` for LDAP users.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nAuthentication Result\\n\\n</td>\\n<td headers=\"Description\">\\nThe success or failure of the authentication request. A reason code is displayed for the failed authentication requests.\\n\\n</td>\\n</tr>\\n</table>Authentication requests are pre-sorted by descending timestamp with 25 entries per page.\\n\\n### Logging Limitations\\n\\nIf a cluster experiences an activity spike and generates an extremely large quantity of log messages, Atlas may stop collecting and storing new logs for a period of time.\\n\\nLog analysis rate limits apply only to the Performance Advisor UI, the Query Insights UI, the Access Tracking UI, and the Atlas Search Query Analytics UI. Downloadable log files are always complete.\\n\\nIf authentication requests occur during a period when logs are not collected, they will not appear in the database access history.\\n\\n## Required Access\\n\\nTo view database access history, you must have `Project Owner` or `Organization Owner` access to Atlas.\\n\\n## Procedure\\n\\n<Tabs>\\n\\n<Tab name=\"Atlas CLI\">\\n\\nTo return the access logs for a cluster using the Atlas CLI, run the following command:\\n\\n```sh\\n\\natlas accessLogs list [options]\\n\\n```\\n\\nTo learn more about the command syntax and parameters, see the Atlas CLI documentation for atlas accessLogs list.\\n\\n- Install the Atlas CLI\\n\\n- Connect to the Atlas CLI\\n\\n</Tab>\\n\\n<Tab name=\"Atlas Administration API\">\\n\\nTo view the database access history using the API, see Access Tracking.\\n\\n</Tab>\\n\\n<Tab name=\"Atlas UI\">\\n\\nUse the following procedure to view your database access history using the Atlas UI:\\n\\n### Navigate to the Clusters page for your project.\\n\\n- If it is not already displayed, select the organization that contains your desired project from the  Organizations menu in the navigation bar.\\n\\n- If it is not already displayed, select your desired project from the Projects menu in the navigation bar.\\n\\n- If the Clusters page is not already displayed, click Database in the sidebar.\\n\\n### View the cluster\\'s database access history.\\n\\n- On the cluster card, click .\\n\\n- Select View Database Access History.\\n\\nor\\n\\n- Click the cluster name.\\n\\n- Click .\\n\\n- Select View Database Access History.\\n\\n</Tab>\\n\\n</Tabs>\\n\\n',\n",
       " 'url': 'https://mongodb.com/docs/atlas/access-tracking/',\n",
       " 'format': 'md',\n",
       " 'title': 'View Database Access History'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview a document to understand its structure\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Chunk up the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common list of separators for text data\n",
    "separators = [\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `RecursiveCharacterTextSplitter` from LangChain to first split a piece of text on the list of `separators` above.\n",
    "# Then recursively merge them into tokens until the specified chunk size is reached.\n",
    "# For text data, you typically want to keep 1-2 paragraphs (~200 tokens) in a single chunk.\n",
    "# Chunk overlap of 15-20% of the chunk size is recommended to maintain context between chunks.\n",
    "# Pass the `separators` list above as an argument called `separators`\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\", separators=separators, chunk_size=200, chunk_overlap=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(doc: Dict, text_field: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunk up a document.\n",
    "\n",
    "    Args:\n",
    "        doc (Dict): Parent document to generate chunks from.\n",
    "        text_field (str): Text field to chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of chunked documents.\n",
    "    \"\"\"\n",
    "    # Extract the field to chunk from `doc`\n",
    "    text = doc[text_field]\n",
    "    # Split `text` using the appropriate method of the `RecursiveCharacterTextSplitter` class\n",
    "    # NOTE: `text` is a string\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    # Iterate through `chunks` and for each chunk:\n",
    "    # 1. Create a shallow copy of `doc`, call it `temp`\n",
    "    # 2. Set the `text_field` field in `temp` to the content of the chunk\n",
    "    # 3. Append `temp` to `chunked_data`\n",
    "    chunked_data = []\n",
    "    for chunk in chunks:\n",
    "        temp = doc.copy()\n",
    "        temp[text_field]=chunk\n",
    "        chunked_data.append(temp)\n",
    "\n",
    "    return chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = []\n",
    "# Iterate through `docs`, use the `get_chunks` function to chunk up the \"body\" field in the documents, and add the list of chunked documents to `split_docs` initialized above.\n",
    "for doc in docs:\n",
    "    chunks = get_chunks(doc, \"body\")\n",
    "    split_docs.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that the length of `split_docs` is greater than the length of `docs` from Step 2 above\n",
    "# This is because each document in `docs` has been split into multiple chunks\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updated': '2024-05-20T17:30:49.148Z',\n",
       " 'metadata': {'contentType': None,\n",
       "  'productName': 'MongoDB Atlas',\n",
       "  'tags': ['atlas', 'docs'],\n",
       "  'version': None},\n",
       " 'action': 'created',\n",
       " 'sourceName': 'snooty-cloud-docs',\n",
       " 'body': '# View Database Access History\\n\\n- This feature is not available for `M0` free clusters, `M2`, and `M5` clusters. To learn more, see Atlas M0 (Free Cluster), M2, and M5 Limits.\\n\\n- This feature is not supported on Serverless instances at this time. To learn more, see Serverless Instance Limitations.\\n\\n## Overview\\n\\nAtlas parses the MongoDB database logs to collect a list of authentication requests made against your clusters through the following methods:\\n\\n- `mongosh`\\n\\n- Compass\\n\\n- Drivers\\n\\nAuthentication requests made with API Keys through the Atlas Administration API are not logged.\\n\\nAtlas logs the following information for each authentication request within the last 7 days:\\n\\n<table>\\n<tr>\\n<th id=\"Field\">\\nField\\n\\n</th>\\n<th id=\"Description\">\\nDescription\\n\\n</th>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nTimestamp',\n",
       " 'url': 'https://mongodb.com/docs/atlas/access-tracking/',\n",
       " 'format': 'md',\n",
       " 'title': 'View Database Access History'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview a chunked document to understand its structure\n",
    "# Note that the structure looks similar to the original docs, except the `body` field now contains smaller chunks of text\n",
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may see a warning upon running this cell. You can ignore it.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the `gte-small` model using the Sentence Transformers library\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š https://huggingface.co/thenlper/gte-small#usage (See \"Use with sentence-transformers\" under Usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a piece of text (`text`) as input, embeds it using the `embedding_model` instantiated above and returns the embedding as a list\n",
    "# An array can be converted to a list using the `tolist()` method\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate the embedding for a piece of text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to embed.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Embedding of the text as a list.\n",
    "    \"\"\"\n",
    "    embedding = embedding_model.encode(text)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107/107 [00:07<00:00, 13.82it/s]\n"
     ]
    }
   ],
   "source": [
    "embedded_docs = []\n",
    "# Add an `embedding` field to each dictionary in `split_docs`\n",
    "# The `embedding` field should correspond to the embedding of the value of the `body` field\n",
    "# Use the `get_embedding` function defined above to generate the embedding\n",
    "# Append the updated dictionaries to `embedded_docs` initialized above.\n",
    "for doc in tqdm(split_docs):\n",
    "    doc[\"embedding\"] = get_embedding(doc[\"body\"])\n",
    "    embedded_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the length of `embedded_docs` is the same as that of `split_docs`\n",
    "len(embedded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Ingest data into MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the database -- Change if needed or leave as is\n",
    "DB_NAME = \"mongodb_genai_devday\"\n",
    "# Name of the collection -- Change if needed or leave as is\n",
    "COLLECTION_NAME = \"knowledge_base\"\n",
    "# Name of the vector search index -- Change if needed or leave as is\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š https://pymongo.readthedocs.io/en/stable/tutorial.html#getting-a-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the `DB_NAME` database.\n",
    "# Use the `mongodb_client` instantiated in Step 1.\n",
    "db = mongodb_client[DB_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š https://pymongo.readthedocs.io/en/stable/tutorial.html#getting-a-collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the `COLLECTION_NAME` collection.\n",
    "# Use the `db` and collection name defined above.\n",
    "collection = db[COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 107, 'electionId': ObjectId('7fffffff0000000000000105'), 'opTime': {'ts': Timestamp(1740604393, 31), 't': 261}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1740604393, 31), 'signature': {'hash': b'[,FT\\xff@\\\\\\x17\\xebah\\xd3/G#\\xb9\\xff\\x0c~c', 'keyId': 7448991076871831556}}, 'operationTime': Timestamp(1740604393, 31)}, acknowledged=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bulk delete all existing records from the collection defined above\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š https://pymongo.readthedocs.io/en/stable/examples/bulk.html#bulk-insert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 107 documents into the knowledge_base collection.\n"
     ]
    }
   ],
   "source": [
    "# Bulk insert `embedded_docs` into the collection defined above -- should be a one-liner\n",
    "collection.insert_many(embedded_docs)\n",
    "\n",
    "print(f\"Ingested {collection.count_documents({})} documents into the {COLLECTION_NAME} collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Create a vector search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index definition specifying:\n",
    "# path: Path to the embeddings field\n",
    "# numDimensions: Number of embedding dimensions- depends on the embedding model used\n",
    "# similarity: Similarity metric. One of cosine, euclidean, dotProduct.\n",
    "model = {\n",
    "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 384,\n",
    "                \"similarity\": \"cosine\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_search_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vector_index'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector search index with the above definition for the `collection` collection\n",
    "collection.create_search_index(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Perform vector search on your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a vector search function\n",
    "\n",
    "ðŸ“š https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Basic Example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve relevant documents for a user query using vector search\n",
    "def vector_search(user_query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents for a user query using vector search.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the `user_query` using the `get_embedding` function defined in Step 4\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    # Define an aggregation pipeline consisting of a $vectorSearch stage, followed by a $project stage\n",
    "    # Set the number of candidates to 150 and only return the top 5 documents from the vector search\n",
    "    # In the $project stage, exclude the `_id` field and include only the `body` field and `vectorSearchScore`\n",
    "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
    "    pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"path\": \"embedding\",\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "    # Execute the aggregation `pipeline` and store the results in `results`\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run vector search queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': '# Backup and Restore Sharded Clusters\\n\\nThe following tutorials describe backup and restoration for sharded clusters:\\n\\nTo use `mongodump` and `mongorestore` as a backup strategy for sharded clusters, you must stop the sharded cluster balancer and use the `fsync` command or the `db.fsyncLock()` method on `mongos` to block writes on the cluster during backups.\\n\\nSharded clusters can also use one of the following coordinated backup and restore processes, which maintain the atomicity guarantees of transactions across shards:\\n\\n- MongoDB Atlas\\n\\n- MongoDB Cloud Manager\\n\\n- MongoDB Ops Manager\\n\\nUse file system snapshots back up each component in the sharded cluster individually. The procedure involves stopping the cluster balancer. If your system configuration allows file system backups, this might be more efficient than using MongoDB tools.\\n\\nCreate backups using `mongodump` to back up each component in the cluster individually.',\n",
       "  'score': 0.9431124925613403},\n",
       " {'body': 'Create backups using `mongodump` to back up each component in the cluster individually.\\n\\nLimit the operation of the cluster balancer to provide a window for regular backup operations.\\n\\nAn outline of the procedure and consideration for restoring an *entire* sharded cluster from backup.',\n",
       "  'score': 0.9327978491783142},\n",
       " {'body': \"# Configuration and Maintenance\\n\\nThis section describes routine management operations, including updating your MongoDB deployment's configuration.\\n\\nOutlines common MongoDB configurations and examples of best-practice configurations for common use cases.\\n\\nUpgrade a MongoDB deployment to a different patch release within the same major release series.\\n\\nStart, configure, and manage running `mongod` process.\\n\\nStop in progress MongoDB client operations using `db.killOp()` and `maxTimeMS()`.\\n\\nArchive the current log files and start new ones.\",\n",
       "  'score': 0.9310877323150635},\n",
       " {'body': '## Full Time Diagnostic Data Capture\\n\\nTo help MongoDB engineers analyze server behavior, `mongod` and `mongos` processes include a Full Time Diagnostic Data Capture (FTDC) mechanism. FTDC is enabled by default. Due to its importance in debugging deployments, FTDC thread failures are fatal and stop the parent `mongod` or `mongos` process.\\n\\nFTDC data files are compressed and not human-readable. They inherit the same file access permissions as the MongoDB data files. Only users with access to FTDC data files can transmit the FTDC data.\\n\\nMongoDB engineers cannot access FTDC data without explicit permission and assistance from system owners or operators.\\n\\nFTDC data **never** contains any of the following information:\\n\\n- Samples of queries, query predicates, or query results\\n\\n- Data sampled from any end-user collection or index\\n\\n- System or MongoDB user credentials or security certificates',\n",
       "  'score': 0.9286065101623535},\n",
       " {'body': \"# Create a MongoDB Deployment\\n\\nYou can create a free tier MongoDB deployment on MongoDB Atlas to store and manage your data. MongoDB Atlas hosts and manages your MongoDB database in the cloud.\\n\\n## Create a Free MongoDB deployment on Atlas\\n\\nComplete the Get Started with Atlas guide to set up a new Atlas account and load sample data into a new free tier MongoDB deployment.\\n\\n## Save your Credentials\\n\\nAfter you create your database user, save that user's username and password to a safe location for use in an upcoming step.\\n\\nAfter you complete these steps, you have a new free tier MongoDB deployment on Atlas, database user credentials, and sample data loaded in your database.\\n\\nIf you run into issues on this step, ask for help in the MongoDB Community Forums or submit feedback by using the Rate this page tab on the right or bottom right side of this page.\",\n",
       "  'score': 0.9270492792129517}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search(\"How to resolve alerts in MongoDB?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦¹â€â™€ï¸ Combine pre-filtering with vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for documents where the product name is `MongoDB Atlas`\n",
    "\n",
    "ðŸ“š https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#about-the-filter-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the vector search index `model` from Step 6 to include the `metadata.productName` field as a `filter` field\n",
    "model = {\n",
    "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 384,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            {\"type\": \"filter\", \"path\": \"metadata.productName\"}\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vector search index\n",
    "collection.update_search_index(\n",
    "    name=ATLAS_VECTOR_SEARCH_INDEX_NAME, definition=model[\"definition\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Check that the index update is complete before proceeding. To do so, ensure that the status of the index shows \"Ready\" in the Atlas UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the user query\n",
    "query_embedding = get_embedding(\n",
    "    \"What are some best practices for data backups in MongoDB?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Filter Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where the `metadata.productName` field has the value \"MongoDB Atlas\"\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5,\n",
    "            \"filter\": {\"metadata.productName\": \"MongoDB Atlas\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the aggregation pipeline and view the results\n",
    "results = collection.aggregate(pipeline)\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter on documents which have been updated on or after `2024-05-19` and where the content type is `Tutorial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the vector search index `model` from Step 6 to include `metadata.contentType` and `updated` as `filter` fields\n",
    "model = {\n",
    "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 384,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            {\"type\": \"filter\", \"path\": \"metadata.contentType\"},\n",
    "            {\"type\": \"filter\", \"path\": \"updated\"}\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vector search index\n",
    "collection.update_search_index(\n",
    "    name=ATLAS_VECTOR_SEARCH_INDEX_NAME, definition=model[\"definition\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Check that the index update is complete before proceeding. To do so, ensure that the status of the index shows \"Ready\" in the Atlas UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the user query\n",
    "query_embedding = get_embedding(\n",
    "    \"What are some best practices for data backups in MongoDB?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where\n",
    "# the `metadata.contentType` field has the value \"Tutorial\"\n",
    "# AND\n",
    "# the `updated` field is greater than or equal to \"2024-05-19\"\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5,\n",
    "            \"filter\": {\n",
    "                \"$and\": [\n",
    "                    {\"metadata.contentType\": \"Tutorial\"},\n",
    "                    {\"updated\": {\"$gte\": \"2024-05-19\"}}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"updated\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationFailure",
     "evalue": "PlanExecutor error during aggregation :: caused by :: Path 'metadata.contentType' needs to be indexed as token, full error: {'ok': 0.0, 'errmsg': \"PlanExecutor error during aggregation :: caused by :: Path 'metadata.contentType' needs to be indexed as token\", 'code': 8, 'codeName': 'UnknownError', '$clusterTime': {'clusterTime': Timestamp(1740604776, 2), 'signature': {'hash': b'\\xb9\\x97\\x82S\\xf1\\xfbO\\xf7&\\xb4-\\x98\\tX\\xdb\\xd1\\x82C\\xcd\\xb4', 'keyId': 7448991076871831556}}, 'operationTime': Timestamp(1740604776, 2)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationFailure\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Execute the aggregation pipeline and view the results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlist\u001b[39m(results)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/synchronous/collection.py:2937\u001b[0m, in \u001b[0;36mCollection.aggregate\u001b[0;34m(self, pipeline, session, let, comment, **kwargs)\u001b[0m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform an aggregation using the aggregation framework on this\u001b[39;00m\n\u001b[1;32m   2862\u001b[0m \u001b[38;5;124;03mcollection.\u001b[39;00m\n\u001b[1;32m   2863\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2934\u001b[0m \u001b[38;5;124;03m    https://mongodb.com/docs/manual/reference/command/aggregate\u001b[39;00m\n\u001b[1;32m   2935\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_database\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39m_tmp_session(session, close\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[0;32m-> 2937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2938\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_CollectionAggregationCommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCommandCursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2941\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplicit_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/_csot.py:120\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/synchronous/collection.py:2845\u001b[0m, in \u001b[0;36mCollection._aggregate\u001b[0;34m(self, aggregation_command, pipeline, cursor_class, session, explicit_session, let, comment, **kwargs)\u001b[0m\n\u001b[1;32m   2834\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m comment\n\u001b[1;32m   2835\u001b[0m cmd \u001b[38;5;241m=\u001b[39m aggregation_command(\n\u001b[1;32m   2836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2837\u001b[0m     cursor_class,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2842\u001b[0m     user_fields\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcursor\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirstBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}},\n\u001b[1;32m   2843\u001b[0m )\n\u001b[0;32m-> 2845\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_read_preference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   2848\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_performs_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2850\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_Op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAGGREGATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2851\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/synchronous/mongo_client.py:1863\u001b[0m, in \u001b[0;36mMongoClient._retryable_read\u001b[0;34m(self, func, read_pref, session, operation, address, retryable, operation_id)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;66;03m# Ensure that the client supports retrying on reads and there is no session in\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;66;03m# transaction, otherwise, we will not support retry behavior for this call.\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m retryable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\n\u001b[1;32m   1861\u001b[0m     retryable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mretry_reads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (session \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39min_transaction)\n\u001b[1;32m   1862\u001b[0m )\n\u001b[0;32m-> 1863\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_read\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/_csot.py:120\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/synchronous/mongo_client.py:1830\u001b[0m, in \u001b[0;36mMongoClient._retry_internal\u001b[0;34m(self, func, session, bulk, operation, is_read, address, read_pref, retryable, operation_id)\u001b[0m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;129m@_csot\u001b[39m\u001b[38;5;241m.\u001b[39mapply\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_retry_internal\u001b[39m(\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1805\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal retryable helper for all client transactions.\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m \n\u001b[1;32m   1808\u001b[0m \u001b[38;5;124;03m    :param func: Callback function we want to retry\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1817\u001b[0m \u001b[38;5;124;03m    :return: Output of the calling func()\u001b[39;00m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ClientConnectionRetryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmongo_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbulk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_read\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1827\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1830\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/synchronous/mongo_client.py:2554\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2552\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error(check_csot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2553\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_read \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write()\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[1;32m   2558\u001b[0m     \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/synchronous/mongo_client.py:2697\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable._read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrying \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retryable:\n\u001b[1;32m   2696\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error()\n\u001b[0;32m-> 2697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/synchronous/aggregation.py:164\u001b[0m, in \u001b[0;36m_AggregationCommand.get_cursor\u001b[0;34m(self, session, server, conn, read_preference)\u001b[0m\n\u001b[1;32m    161\u001b[0m     write_concern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Run command.\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_processor:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_processor(result, conn)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/synchronous/helpers.py:45\u001b[0m, in \u001b[0;36m_handle_reauth.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynchronous\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Connection\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailure \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_reauth:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/synchronous/pool.py:538\u001b[0m, in \u001b[0;36mConnection.command\u001b[0;34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_not_writable(unacknowledged)\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_mongos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallowable_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlisteners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bson_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_op_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop_msg_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43munacknowledged\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munacknowledged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexhaust_allowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexhaust_allowed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (OperationFailure, NotPrimaryError):\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/synchronous/network.py:218\u001b[0m, in \u001b[0;36mcommand\u001b[0;34m(conn, dbname, spec, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields, exhaust_allowed, write_concern)\u001b[0m\n\u001b[1;32m    216\u001b[0m             client\u001b[38;5;241m.\u001b[39m_process_response(response_doc, session)\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m check:\n\u001b[0;32m--> 218\u001b[0m             \u001b[43mhelpers_shared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_command_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m                \u001b[49m\u001b[43mresponse_doc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_wire_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowable_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    225\u001b[0m     duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pymongo/helpers_shared.py:247\u001b[0m, in \u001b[0;36m_check_command_response\u001b[0;34m(response, max_wire_version, allowable_errors, parse_write_concern_error)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m43\u001b[39m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CursorNotFound(errmsg, code, response, max_wire_version)\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OperationFailure(errmsg, code, response, max_wire_version)\n",
      "\u001b[0;31mOperationFailure\u001b[0m: PlanExecutor error during aggregation :: caused by :: Path 'metadata.contentType' needs to be indexed as token, full error: {'ok': 0.0, 'errmsg': \"PlanExecutor error during aggregation :: caused by :: Path 'metadata.contentType' needs to be indexed as token\", 'code': 8, 'codeName': 'UnknownError', '$clusterTime': {'clusterTime': Timestamp(1740604776, 2), 'signature': {'hash': b'\\xb9\\x97\\x82S\\xf1\\xfbO\\xf7&\\xb4-\\x98\\tX\\xdb\\xd1\\x82C\\xcd\\xb4', 'keyId': 7448991076871831556}}, 'operationTime': Timestamp(1740604776, 2)}"
     ]
    }
   ],
   "source": [
    "# Execute the aggregation pipeline and view the results\n",
    "results = collection.aggregate(pipeline)\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Build the RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to create the chat prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the user prompt for our RAG application\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 7\n",
    "    context = vector_search(user_query)\n",
    "    # Join the retrieved documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
    "    context = \"\\n\\n\".join([doc.get('body') for doc in context])\n",
    "    # Prompt consisting of the question and relevant context to answer it\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to answer user queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to answer user queries\n",
    "def generate_answer(user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Use the `create_prompt` function above to create a chat prompt\n",
    "    prompt = create_prompt(user_query)\n",
    "    # Format the message to the LLM in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
    "    # The role value for user messages must be \"user\"\n",
    "    # Use the `prompt` created above to populate the `content` field in the chat message\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # Send the chat messages to a serverless function to get back an LLM response\n",
    "    response = requests.post(url=SERVERLESS_URL, json={\"task\": \"completion\", \"data\": messages})\n",
    "    # Print the final answer\n",
    "    print(response.json()[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, some best practices for data backups in MongoDB sharded clusters include:\n",
      "\n",
      "1. Stopping the sharded cluster balancer before performing backups.\n",
      "\n",
      "2. Using the `fsync` command or `db.fsyncLock()` method on `mongos` to block writes during backups when using `mongodump` and `mongorestore`.\n",
      "\n",
      "3. Using coordinated backup and restore processes that maintain atomicity guarantees across shards, such as MongoDB Atlas, MongoDB Cloud Manager, or MongoDB Ops Manager.\n",
      "\n",
      "4. Using file system snapshots to back up each component in the sharded cluster individually, if the system configuration allows.\n",
      "\n",
      "5. Using `mongodump` to back up each component in the cluster individually.\n",
      "\n",
      "6. Limiting the operation of the cluster balancer to provide a window for regular backup operations.\n",
      "\n",
      "7. For full sharded cluster backups, following a specific procedure and considerations for restoring the entire cluster.\n",
      "\n",
      "The context also mentions that it's important to consider the atomicity guarantees of transactions across shards when choosing a backup method.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I DON'T KNOW\n",
      "\n",
      "The given context does not contain any question that you asked me. The context provides information about using CodeWhisperer, requesting code suggestions, and some JSON data related to reviews. There is no previous question mentioned in this context.\n"
     ]
    }
   ],
   "source": [
    "# Notice that the LLM does not remember the conversation history at this stage\n",
    "generate_answer(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦¹â€â™€ï¸ Re-rank retrieved results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1#quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a re-ranking step to the following function\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 7\n",
    "    context = vector_search(user_query)\n",
    "    # Extract the \"body\" field from each document in `context`\n",
    "    documents = [d.get(\"body\") for d in context]\n",
    "    # Use the `rerank_model` instantiated above to re-rank `documents`\n",
    "    # Set the `top_k` argument to 5 \n",
    "    reranked_documents = rerank_model.rank(\n",
    "    user_query, documents, return_documents=True, top_k=5\n",
    ")\n",
    "    # Join the re-ranked documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
    "    context = \"\\n\\n\".join([d.get(\"text\", \"\") for d in reranked_documents])\n",
    "    # Prompt consisting of the question and relevant context to answer it\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, some best practices for data backups in MongoDB sharded clusters include:\n",
      "\n",
      "1. Stopping the sharded cluster balancer before performing backups.\n",
      "\n",
      "2. Using the `fsync` command or `db.fsyncLock()` method on `mongos` to block writes during backups when using `mongodump` and `mongorestore`.\n",
      "\n",
      "3. Using coordinated backup and restore processes that maintain atomicity guarantees across shards, such as MongoDB Atlas, MongoDB Cloud Manager, or MongoDB Ops Manager.\n",
      "\n",
      "4. Using file system snapshots to back up each component in the sharded cluster individually, if the system configuration allows.\n",
      "\n",
      "5. Using `mongodump` to back up each component in the cluster individually.\n",
      "\n",
      "6. Limiting the operation of the cluster balancer to provide a window for regular backup operations.\n",
      "\n",
      "7. For full sharded cluster backups, following a specific procedure and considerations for restoring the entire cluster.\n",
      "\n",
      "The context also mentions that it's important to consider the atomicity guarantees of transactions across shards when choosing a backup method.\n"
     ]
    }
   ],
   "source": [
    "# Note the impact of re-ranking on the generated answer\n",
    "# You might not see a difference in this example since we are only re-ranking 5 documents\n",
    "# In practice, you would send a larger number of documents to the re-ranker, and get the top few AFTER reranking\n",
    "generate_answer(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Add memory to the RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_collection = mongodb_client[DB_NAME][\"chat_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'session_id_1'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an index on the key `session_id` for the `history_collection` collection\n",
    "history_collection.create_index(\"session_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to store chat messages in MongoDB\n",
    "\n",
    "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chat_message(session_id: str, role: str, content: str) -> None:\n",
    "    \"\"\"\n",
    "    Store a chat message in a MongoDB collection.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID of the message.\n",
    "        role (str): Role for the message. One of `system`, `user` or `assistant`.\n",
    "        content (str): Content of the message.\n",
    "    \"\"\"\n",
    "    # Create a message object with `session_id`, `role`, `content` and `timestamp` fields\n",
    "    # `timestamp` should be set the current timestamp\n",
    "    message = {\n",
    "        \"session_id\": session_id,\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": datetime.now(),\n",
    "    }\n",
    "    # Insert the `message` into the `history_collection` collection\n",
    "    history_collection.insert_one(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to retrieve chat history from MongoDB\n",
    "\n",
    "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_history(session_id: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve chat message history for a particular session.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat message history for.\n",
    "\n",
    "    Returns:\n",
    "        List: List of chat messages.\n",
    "    \"\"\"\n",
    "    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n",
    "    # Sort the results in increasing order of the values in `timestamp` field\n",
    "    cursor =  history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
    "\n",
    "    if cursor:\n",
    "        # Iterate through the cursor and extract the `role` and `content` field from each entry\n",
    "        # Then format each entry as: {\"role\": <role_value>, \"content\": <content_value>}\n",
    "        messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in cursor]\n",
    "    else:\n",
    "        # If cursor is empty, return an empty list\n",
    "        messages = []\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle chat history in the `generate_answer` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(session_id: str, user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user's query taking chat history into account.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat history for.\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Initialize list of messages to pass to the chat completion model\n",
    "    messages = []\n",
    "\n",
    "    # Retrieve documents relevant to the user query and convert them to a single string\n",
    "    context = vector_search(user_query)\n",
    "    context = \"\\n\\n\".join([d.get(\"body\", \"\") for d in context])\n",
    "    # Create a system prompt containing the retrieved context\n",
    "    system_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\",\n",
    "    }\n",
    "    # Append the system prompt to the `messages` list\n",
    "    messages.append(system_message)\n",
    "\n",
    "    # Use the `retrieve_session_history` function to retrieve message history from MongoDB for the session ID `session_id` \n",
    "    # And add all messages in the message history to the `messages` list \n",
    "    message_history = retrieve_session_history(session_id)\n",
    "    messages.extend(message_history)\n",
    "\n",
    "    # Format the user query in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
    "    # The role value for user messages must be \"user\"\n",
    "    # And append the user message to the `messages` list\n",
    "    user_message = {\"role\": \"user\", \"content\": user_query}\n",
    "    messages.append(user_message)\n",
    "\n",
    "    # Send the chat messages to a serverless function to get back an LLM response\n",
    "    response = requests.post(url=SERVERLESS_URL, json={\"task\": \"completion\", \"data\": messages})\n",
    "\n",
    "    # Extract the answer from the response\n",
    "    answer = response.json()[\"text\"]\n",
    "\n",
    "    # Use the `store_chat_message` function to store the user message and also the generated answer in the message history collection\n",
    "    # The role value for user messages is \"user\", and \"assistant\" for the generated answer\n",
    "    store_chat_message(session_id, \"user\", user_query)\n",
    "    store_chat_message(session_id, \"assistant\", answer)\n",
    "\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, some best practices for data backups in MongoDB sharded clusters include:\n",
      "\n",
      "1. Stopping the sharded cluster balancer before performing backups.\n",
      "\n",
      "2. Using the fsync command or db.fsyncLock() method on mongos to block writes during backups when using mongodump and mongorestore.\n",
      "\n",
      "3. Using coordinated backup and restore processes that maintain transaction atomicity across shards, such as MongoDB Atlas, MongoDB Cloud Manager, or MongoDB Ops Manager.\n",
      "\n",
      "4. Using file system snapshots to back up each component of the sharded cluster individually, if the system configuration allows.\n",
      "\n",
      "5. Using mongodump to back up each component of the cluster individually.\n",
      "\n",
      "6. Limiting the operation of the cluster balancer to provide a window for regular backup operations.\n",
      "\n",
      "7. For full cluster backups, following a specific procedure to restore the entire sharded cluster from backup.\n",
      "\n",
      "The context also mentions that it's important to consider the atomicity guarantees of transactions across shards when choosing a backup method.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What are some best practices for data backups in MongoDB?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked: \"What are some best practices for data backups in MongoDB?\"\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What did I just ask you?\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
